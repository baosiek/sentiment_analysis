{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bittlcondac3b95453ae5d46cea17442527270ea62",
   "display_name": "Python 3.7.9 64-bit ('tl': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h1><center>Sentiment Analysis Classifier with BERT</center></h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This project is based on Manning's book \"Transfer Learning for NLP\" (chapter 2).\n",
    "The goal here is:\n",
    "\n",
    "1. Curate a dataset with reviews from imdb classic dataset\n",
    "2. Create a Pandas dataframe from iy\n",
    "3. Create a simple bag-of-words model from the above content. Simple because it is based on term frequency (tf) only.\n",
    "4. Choose one baseline classifier from Logistic Regression and Gradient Boosting Machine\n",
    "5. Accuracy is the metric of choice as the dataset is balanced and consists of two classes\n",
    "6. Train a SentimentAnalysis classifier based on BERT embeddings\n",
    "\n",
    "But before starting let's make sure we have the correct libraries versions installed, namelly tensorflow and bert-tensorflow."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tensorflow==1.15.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: bert-tensorflow==1.0.1 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.31.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.0.8)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.19.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.35.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.15.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (3.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.11.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (3.3.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (50.3.1.post20201107)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (0.16.1)\n",
      "Requirement already satisfied: h5py in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.0->-r requirements.txt (line 1)) (2.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (3.4.0)\n"
     ]
    }
   ]
  },
  {
   "source": [
    "Now we will import required Python libraries and the dataset. To download the dataset I will use the bash script get_aclImdb.sh. It downloads and extracts the compressed archive into ./data/aclImdb. It requires execution privilege (sudo chmod +x get_aclImdb.sh)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading aclImdb.\n",
      "--2020-11-25 10:52:25--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  2.19MB/s    in 31s     \n",
      "\n",
      "2020-11-25 10:52:56 (2.59 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n",
      "aclImdb downloaded and extracted into ./data/aclImdb.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pickle as pck \n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "# download dataset\n",
    "!./get_aclImdb.sh"
   ]
  },
  {
   "source": [
    "# Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 256 # maximum number of tokens per review\n",
    "max_chars = 20 # maximum size of a token."
   ]
  },
  {
   "source": [
    "Three helper methods to tokenize, remoce stopwords, remove puntuation and convert to lowercase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    if text==None or text=='' or type(text)=='list':\n",
    "        tokens = \"\"\n",
    "    else:\n",
    "        tokens = text.split(' ')[: max_tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/baosiek/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def stopword_removal(tokens):\n",
    "\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    filtered_tokens = filter(None, filtered_tokens)\n",
    "    \n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        token = re.sub(r'[\\W\\d]', \"\", token)[:max_chars]\n",
    "        cleaned_tokens.append(token)\n",
    "\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(reviews_np, sentiment_list):\n",
    "\n",
    "    shuffle_index = np.random.permutation(len(sentiment_list))\n",
    "    reviews_np = reviews_np[shuffle_index]\n",
    "    sentiments_np = np.asarray(sentiment_list)[shuffle_index]\n",
    "\n",
    "    return reviews_np, sentiments_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "\n",
    "    reviews, sentiments = [], []\n",
    "    for folder, sentiment in (('neg', 0), ('pos', 1)):\n",
    "        folder = os.path.join(path, folder)\n",
    "        for name in os.listdir(folder):\n",
    "            with open(os.path.join(folder, name), 'r') as reader:\n",
    "                text = reader.read()\n",
    "\n",
    "            text = tokenize(text)\n",
    "            text = stopword_removal(text)\n",
    "            text = clean_tokens(text)\n",
    "\n",
    "            reviews.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "\n",
    "    reviews_np = np.array(reviews)\n",
    "    reviews_np, sentiments_np = shuffle_data(reviews_np, sentiments)\n",
    "\n",
    "    return reviews_np, sentiments_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[list(['i', 'wonder', 'audiences', 'day', 'thought', 'first', 'laying', 'eyes', 'walter', 'jack', 'palance', 'blackie', 'certainly', 'looks', 'like', 'one', 'else', 'time', 'skulllike', 'face', 'flattened', 'nose', 'elongated', 'body', 'even', 'remains', 'unsettling', 'presence', 'and', 'could', 'appropriate', 'emergence', 'dingy', 'new', 'orleans', 'slums', 'appear', 'fester', 'like', 'plague', 'blackie', 'loosing', 'city', 'im', 'sorry', 'scenesbr', 'br', 'the', 'movie', 'skillfully', 'assembled', 'morgues', 'black', 'humor', 'widmark', 'douglas', 'interplay', 'untouristy', 'locations', 'battles', 'among', 'officialsall', 'woven', 'tensely', 'realistic', 'thriller', 'menace', 'unlike', 'others', 'time', 'even', 'widmarks', 'domestic', 'scenes', 'put', 'woman', 'bel', 'geddes', 'marquee', 'manage', 'disruptive', 'director', 'kazan', 'certainly', 'shows', 'aptitude', 'helming', 'studio', 'fox', 'product', 'matter', 'may', 'felt', 'commercial', 'aspectbr', 'br', 'widmark', 'solid', 'lowkey', 'job', 'public', 'health', 'officer', 'but', 'money', 'one', 'andonly', 'zero', 'mostel', 'was', 'ever', 'sweatier', 'performer', 'could', 'squeal', 'louder', 'get', 'pushed', 'around', 'bulky', 'fall', 'guy', 'eg', 'the', 'enforcer', '', 'that', 'scene', 'mostel', 'palance', 'argues', 'mostels', 'shrewish', 'wife', 'liswood', 'gem', 'frantic', 'subservience', 'mostel', 'tries', 'pacify', 'like', 'berserk', 'pinball', 'too', 'bad', 'lost', 'many', 'years', 'blacklist', 'i', 'wonder', 'voluble', 'kazan', 'named', 'himbr', 'br', 'but'])\n list(['the', 'five', 'deadly', 'venoms', 'easily', 'memorable', 'kf', 'flick', 'shaw', 'brothers', 'stable', 'exc', 'maybe', 'master', 'killer', 'it', 'artists', 'came', 'known', 'simply', 'the', 'venoms', 'best', 'fight', 'choreography', 'ever', 'and', 'surprisingly', 'kung', 'fu', 'movie', 'great', 'plot', 'one', 'alltime', 'favorites'])\n list(['thank', 'god', 'abc', 'picked', 'instead', 'fox', 'the', 'best', 'description', 'for', 'know', 'really', 'wonderfalls', 'meets', 'dead', 'like', 'me', 'best', 'way', 'possiblebr', 'br', 'im', 'sure', 'whether', 'experience', 'death', 'destiny', 'early', 'life', 'makes', 'fan', 'brian', 'fuller', 'i', 'certainly', 'enjoy', 'productions', 'i', 'also', 'enjoy', 'checkered', 'floors', 'pies', 'talking', 'toys', 'gravelings', 'mischievous', 'items', '', 'while', 'bit', 'burtonesque', 'i', 'certainly', 'think', 'enjoys', 'niche', 'require', 'j', 'depp', 'hb', 'carter', 'wonderfully', 'imaginative', 'playground', 'here', 'find', 'joys', 'sorrows', 'childhood', 'adulthood', 'crashing', 'actually', 'making', 'sense', 'making', 'us', 'want', 'live', 'life', 'fullest'])\n list(['this', 'film', 'decided', 'go', 'see', 'im', 'huge', 'fan', 'adult', 'animation', 'i', 'quite', 'often', 'find', 'film', 'evolve', 'around', 'famous', 'actor', 'actress', 'rather', 'story', 'style', 'allows', 'film', 'viewed', 'piece', 'art', 'rather', 'showcase', 'actors', 'ability', 'differ', 'stylesbr', 'br', 'this', 'film', 'certainly', 'style', 'story', 'while', 'found', 'story', 'interesting', 'a', 'thriller', 'borrows', 'story', 'atmosphere', 'films', 'blade', 'runner', 'many', 'anime', 'films', 'bit', 'hard', 'follow', 'times', 'feel', 'like', 'came', 'together', 'well', 'could', 'have', 'it', 'definitely', 'mixed', 'sense', 'french', 'animation', 'japanese', 'anime', 'coming', 'together', 'whether', 'thats', 'good', 'thing', 'viewer', 'visually', 'film', 'treat', 'eyes', 'sense', 'work', 'artbr', 'br', 'if', 'like', 'adult', 'animation', 'would', 'like', 'see', 'film', 'different', 'films', 'moment', 'i', 'would', 'recommend', 'it', 'all', 'say', 'enjoyed', 'experience', 'film', 'come', 'away', 'slightly', 'disappointed', 'could', 'better'])\n list(['unlike', 'comments', 'mine', 'positive', 'this', 'movie', 'wraps', 'around', 'dinner', 'table', 'group', 'friends', 'like', 'dont', 'a', 'relatedmother', 'daughter', 'son', 'their', 'stories', 'one', 'smooth', 'happy', 'everyone', 'everything', 'type', 'lifemuch', 'like', 'real', 'life', 'some', 'story', 'lines', 'evolve', 'happen', 'but', 'like', 'true', 'families', 'good', 'friends', 'stick', 'together', 'the', 'wannabe', 'parents', 'buying', 'baby', 'aholes', 'you', 'happy', 'ending', 'poor', 'delmar', 'stuck', 'rock', 'boulder', 'taking', 'care', 'herself', 'mom', 'son', 'trying', 'keep', 'lives', 'together', 'this', 'end', 'sunset', 'walk', 'house', 'burbs', 'living', 'dream', 'world', 'real', 'life', 'portrayal', 'people', 'living', 'day', 'day', 'month', 'month', 'overall', 'good', 'story', 'great', 'movie'])] [1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join('data/aclImdb', 'train')\n",
    "raw_data, raw_label = load_dataset(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}