{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitdeeplearningconda8b405214170f4fc6bc10bfc6973ba42f",
   "display_name": "Python 3.7.9 64-bit ('deep_learning': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h1><center>Sentiment Analysis Classifier with BERT</center></h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This project is based on Manning's book \"Transfer Learning for NLP\" (chapter 2).\n",
    "The goal here is:\n",
    "\n",
    "1. Curate a dataset with reviews from imdb classic dataset\n",
    "2. Create a Pandas dataframe from it.\n",
    "3. Create a simple bag-of-words model from the above content. Simple because it is based on term frequency (tf) only.\n",
    "4. Choose one baseline classifier from Logistic Regression and Gradient Boosting Machine\n",
    "5. Accuracy will be the metric of choice as the dataset is balanced and consists of two classes\n",
    "6. Train a SentimentAnalysis classifier based on BERT embeddings\n",
    "\n",
    "But before starting let's make sure we have the correct libraries versions installed, namelly tensorflow and bert-tensorflow."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tensorflow==1.15.2 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.15.2)\n",
      "Requirement already satisfied: tensorflow-hub==0.9.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: bert-tensorflow==1.0.1 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: keras==2.3.1 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (2.3.1)\n",
      "Requirement already satisfied: tqdm==4.55.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (4.55.0)\n",
      "Requirement already satisfied: six in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from bert-tensorflow==1.0.1->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from keras==2.3.1->-r requirements.txt (line 4)) (1.19.2)\n",
      "Requirement already satisfied: h5py in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from keras==2.3.1->-r requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from keras==2.3.1->-r requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from keras==2.3.1->-r requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from keras==2.3.1->-r requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from keras==2.3.1->-r requirements.txt (line 4)) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.15.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.34.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (49.6.0.post20201009)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: cached-property in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from h5py->keras==2.3.1->-r requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (3.4.0)\n"
     ]
    }
   ]
  },
  {
   "source": [
    "Now we will import required Python libraries and the dataset. To download the dataset I will use the bash script get_aclImdb.sh. It downloads and extracts the compressed archive into ./data/aclImdb. It requires execution privilege (sudo chmod +x get_aclImdb.sh)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "aclImdb already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pickle as pck \n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "# download dataset\n",
    "!./get_aclImdb.sh"
   ]
  },
  {
   "source": [
    "# Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 256 # maximum number of tokens per review\n",
    "max_chars = 20 # maximum size of a token.\n",
    "n_samples = 1000 # number of training instances"
   ]
  },
  {
   "source": [
    "Three helper methods to tokenize, remove stopwords, remove puntuation and convert to lowercase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_tfidf(path):\n",
    "\n",
    "    # Reviews will be loaded in sequence, i.e., first all negative s followed by all positives.\n",
    "    reviews, sentiments = [], []\n",
    "    for folder, sentiment in (('neg', 0), ('pos', 1)):\n",
    "        folder = os.path.join(path, folder)\n",
    "        for name in os.listdir(folder):\n",
    "            with open(os.path.join(folder, name), 'r') as reader:\n",
    "                text = reader.read()\n",
    "\n",
    "            reviews.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "\n",
    "    return reviews, sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training reviews: 25000 and labels: 25000\n",
      "Number of testing reviews: 25000 and labels: 25000\n"
     ]
    }
   ],
   "source": [
    "# Loaded dataset comes tokenized, lowercased and without both stopwords and punctuations\n",
    "\n",
    "train_path = os.path.join('data/aclImdb', 'train')\n",
    "reviews_train, sentiments_train = load_dataset_tfidf(train_path)\n",
    "print(f'Number of training reviews: {len(reviews_train)} and labels: {len(sentiments_train)}')\n",
    "\n",
    "test_path = os.path.join('data/aclImdb', 'test')\n",
    "reviews_test, sentiments_test = load_dataset_tfidf(test_path)\n",
    "print(f'Number of testing reviews: {len(reviews_test)} and labels: {len(sentiments_test)}')"
   ]
  },
  {
   "source": [
    "As seen previously this dataset may be huge to be digested in some environments. So let's limit the train dataset to the hyperparameter n_samples. The idea is to have a dataset with evenly distributed samples from negative and positive labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# BOW for Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_data(data, label):\n",
    "\n",
    "    print(len(label))\n",
    "    data_tmp = np.array(data)\n",
    "    label_tmp = np.asarray(label)\n",
    "    index_shuffle = np.random.permutation(data_tmp.shape[0])\n",
    "    data_tmp = data_tmp[index_shuffle]\n",
    "    label_tmp = label_tmp[index_shuffle]\n",
    "\n",
    "    return data_tmp.tolist(), label_tmp.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25000\n25000\n"
     ]
    }
   ],
   "source": [
    "print(len(sentiments_train))\n",
    "reviews_train, sentiments_train = shuffle_data(reviews_train, sentiments_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = vectorizer.transform(reviews_train)\n",
    "train_y = np.array(sentiments_train)"
   ]
  },
  {
   "source": [
    "# Logistic Regression "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def fit(train_x, train_y):\n",
    "\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    try:\n",
    "        model.fit(train_x, train_y)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000, 74849) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "model = fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = vectorizer.transform(reviews_test)\n",
    "test_y = np.array(sentiments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression accucary is:  0.8832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score = accuracy_score(test_y, predictions)\n",
    "print(f'LogisticRegression accucary is: {accuracy_score: .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "\n",
    "# Making sure we have the correct tensorflow version\n",
    "assert tf.__version__ == \"1.15.2\", 'Correct tensorflow version is 1.15.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting TF sessions and setting it to keras backend\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "class BertEmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "        n_fine_tune_layer = 10,\n",
    "        pooling = 'mean',\n",
    "        bert_path = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1',\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        self.n_fine_tune_layer = n_fine_tune_layer\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "\n",
    "        super(BertEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_features):\n",
    "\n",
    "        self.bert_layer = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f'{self.name}_module')\n",
    "\n",
    "        print(type(self.bert_layer))\n",
    "\n",
    "        trainable_vars = self.bert_layer.variables\n",
    "        \n",
    "        if self.pooling == 'first':\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "        elif self.pooling == 'mean':\n",
    "            trainable_vars = [\n",
    "                var for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name \n",
    "                and not \"/pooler/\" in var.name]\n",
    "\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError('Undefined pooling type')\n",
    "\n",
    "        for i in range(self.n_fine_tune_layer):\n",
    "            trainable_layers.append(f'encoder/layer_{str(11 - i)}')\n",
    "\n",
    "        trainable_vars = [\n",
    "            var for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert_layer.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertEmbeddingLayer, self).build(input_features)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype='int32') for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
    "\n",
    "        if self.pooling == 'first':\n",
    "            pooled = self.bert_layer(\n",
    "                inputs=bert_inputs, signature='tokens', as_dict=True)['pooled_output']\n",
    "\n",
    "        elif self.pooling == 'mean':\n",
    "            result = self.bert_layer(\n",
    "                inputs=bert_inputs, signature='tokens', as_dict=True)['sequence_output']\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (tf.reduce_sum(m, axis=1, keep_dims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "\n",
    "        else:\n",
    "            raise NameError('Undefined pooling type')\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_fine_tune_layer': self.n_fine_tune_layer,\n",
    "            'pooling': self.pooling,\n",
    "            'bert_path': self.bert_path,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000,)\n(25000,)\n(25000, 1)\n(25000, 1)\n"
     ]
    }
   ],
   "source": [
    "def convert_data_to_bert_feature_format(x, y):\n",
    "\n",
    "    # Converted_data to np.array\n",
    "    converted_data = np.array(x, dtype=object)[:, np.newaxis]\n",
    "\n",
    "    return converted_data, np.array(y)\n",
    "\n",
    "data_train, labels_train = convert_data_to_bert_feature_format(reviews_train, sentiments_train)\n",
    "data_test, labels_test = convert_data_to_bert_feature_format(reviews_test, sentiments_test)\n",
    "\n",
    "print(labels_train.shape)\n",
    "print(labels_test.shape)\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_seq_length):\n",
    "\n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name='input_ids')\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name='input_masks')\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name='segment_ids')\n",
    "    bert_layer_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "    bert_output = BertEmbeddingLayer(n_fine_tune_layer=0)(bert_layer_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=bert_layer_inputs, outputs=output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Converting examples to features: 100%|██████████| 25000/25000 [01:36<00:00, 258.01it/s]\n",
      "Converting examples to features: 100%|██████████| 25000/25000 [01:35<00:00, 261.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import kerasBert as kb\n",
    "import bert.tokenization as tk\n",
    "from tensorflow_hub import Module\n",
    "import pkg_resources as pkg\n",
    "import bert_utils as bu\n",
    "\n",
    "# Making sure we have install the correct libraries.\n",
    "assert pkg.get_distribution(\"bert-tensorflow\").version == '1.0.1', 'Change bert-tensorflow version to 1.0.1'\n",
    "\n",
    "vocab_file_path = '/home/baosiek/Development/deep_learning/sentiment_analysis/model/bert/assets/vocab.txt'\n",
    "tokenizer = bu.create_tokenizer(vocab_file_path)\n",
    "\n",
    "train_examples = kb.convert_text_to_examples(data_train, labels_train)\n",
    "test_examples = kb.convert_text_to_examples(data_test, labels_test)\n",
    "\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels) = kb.convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_tokens)\n",
    "\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels) = kb.convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('model/bert/bert_model.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Build and training model!\n",
      "<class 'tensorflow_hub.module.Module'>\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:tensorflow:From /tmp/tmpvl9hyvhw.py:38: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /tmp/tmpvl9hyvhw.py:38: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_embedding_layer (BertEmbed (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_embedding_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 197,121\n",
      "Non-trainable params: 110,104,890\n",
      "__________________________________________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "indices[14,23,0] = 82782 is not in [0, 30522)\n\t [[{{node bert_embedding_layer/bert_embedding_layer_module_apply_tokens/bert/embeddings/embedding_lookup}}]]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-847169475497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Fitting the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_segment_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcp_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_segment_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Saving history dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[14,23,0] = 82782 is not in [0, 30522)\n\t [[{{node bert_embedding_layer/bert_embedding_layer_module_apply_tokens/bert/embeddings/embedding_lookup}}]]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Will train only if bert_model.h5 does not exi\n",
    "if not os.path.exists('model/bert/bert_model.hdf5'):\n",
    "    print('Build and training model!')\n",
    "    model = build_model(max_tokens)\n",
    "\n",
    "    # Initialize keras.backend==tensorflow session\n",
    "    initialize_vars(sess)\n",
    "\n",
    "    # Fitting the model\n",
    "    history = model.fit([train_input_ids, train_input_masks, train_segment_ids], train_labels, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels), epochs=5, batch_size=32)\n",
    "\n",
    "    # Saving history dictionary\n",
    "    history_dict = history.history\n",
    "    tf.keras.models.save_model(model, 'model/bert/bert_model.hdf5')\n",
    "    with open('model/bert/train_history_dict', 'wb') as file_pi:\n",
    "        pickle.dump(history_dict, file_pi)\n",
    "else:\n",
    "    print('Loading trained model!')\n",
    "    model = tf.keras.models.load_model('model/bert/bert_model.hdf5', {'BertEmbeddingLayer': BertEmbeddingLayer})\n",
    "    with open(\"model/bert/train_history_dict\", \"rb\") as fp:\n",
    "        history_dict = pickle.load(fp)\n",
    "\n",
    "# ploting history for accuracy and loss\n",
    "plt.plot(history_dict['acc'])\n",
    "plt.plot(history_dict['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# ploting history for loss\n",
    "plt.plot(history_dict['loss'])\n",
    "plt.plot(history_dict['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Now lets see examples working\n",
    "print('--------------------------------')\n",
    "print('Applying classifier on examples:')\n",
    "print('--------------------------------')\n",
    "\n",
    "text_examples = ['This is an email', 'The Royal family lives at Buckingham Palace', 'Let\\'s schedule the meeting to 2:00PM EST', 'Buy your Diesel watch today and get a 10\\'%\\' discount']\n",
    "\n",
    "# Here I am classifying messages one at a time just to work with different helper functions.\n",
    "# This could be dealt as we did with training however,i.e., using kb.convert_text_to_examples\n",
    "# and then kb.convert_examples_to_features.\n",
    "\n",
    "for text_example in text_examples:\n",
    "    # example should be an instance of class InputExample in keras-bert.py\n",
    "    example = kb.InputExample(None, text_example)\n",
    "\n",
    "    # helpser methos from keras-bert.py\n",
    "    (example_input_id, example_input_mask, example_segment_id, example_label) = kb.convert_single_example(tokenizer, example, max_seq_length=max_tokens)\n",
    "\n",
    "    # predicting\n",
    "    predict = model.predict([np.reshape(example_input_id, (1, len(example_input_id))), np.reshape(example_input_mask, (1, len(example_input_mask))), np.reshape(example_segment_id, (1, len(example_segment_id)))])\n",
    "    if predict[0] < 0.5:\n",
    "        print(f'{example.text_a}: is NEGATIVE!')\n",
    "    else:\n",
    "        print(f'{example.text_a}: is POSITIVE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}