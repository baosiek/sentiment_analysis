{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitdeeplearningconda8b405214170f4fc6bc10bfc6973ba42f",
   "display_name": "Python 3.7.9 64-bit ('deep_learning': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h1><center>Sentiment Analysis Classifier with BERT</center></h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This project is based on Manning's book \"Transfer Learning for NLP\" (chapter 2).\n",
    "The goal here is:\n",
    "\n",
    "1. Curate a dataset with reviews from imdb classic dataset\n",
    "2. Create a Pandas dataframe from it.\n",
    "3. Create a simple bag-of-words model from the above content. Simple because it is based on term frequency (tf) only.\n",
    "4. Choose one baseline classifier from Logistic Regression and Gradient Boosting Machine\n",
    "5. Accuracy will be the metric of choice as the dataset is balanced and consists of two classes\n",
    "6. Train a SentimentAnalysis classifier based on BERT embeddings\n",
    "\n",
    "But before starting let's make sure we have the correct libraries versions installed, namelly tensorflow and bert-tensorflow."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tensorflow==1.15.2 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.15.2)\n",
      "Requirement already satisfied: tensorflow-hub==0.9.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: bert-tensorflow==1.0.1 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: keras==2.3.1 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (2.3.1)\n",
      "Requirement already satisfied: tqdm==4.55.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (4.55.0)\n",
      "Requirement already satisfied: h5py==2.10.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (2.10.0)\n",
      "Requirement already satisfied: six in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from bert-tensorflow==1.0.1->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from h5py==2.10.0->-r requirements.txt (line 6)) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from keras==2.3.1->-r requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: pyyaml in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from keras==2.3.1->-r requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from keras==2.3.1->-r requirements.txt (line 4)) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from keras==2.3.1->-r requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.34.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.15.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (49.6.0.post20201009)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (3.3.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (3.4.0)\n"
     ]
    }
   ]
  },
  {
   "source": [
    "Now we will import required Python libraries and the dataset. To download the dataset I will use the bash script get_aclImdb.sh. It downloads and extracts the compressed archive into ./data/aclImdb. It requires execution privilege (sudo chmod +x get_aclImdb.sh)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./model/bert/assets created!.\n",
      "Downloading aclImdb.\n",
      "--2020-12-29 16:32:09--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  5.65MB/s    in 14s     \n",
      "\n",
      "2020-12-29 16:32:24 (5.54 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n",
      "aclImdb downloaded and extracted into ./data/.\n",
      "Downloading bert_uncased_L-12_H-768_A-12_1.\n",
      "--2020-12-29 16:32:28--  https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1?tf-hub-format=compressed\n",
      "Resolving tfhub.dev (tfhub.dev)... 2607:f8b0:400a:800::200e, 172.217.14.238\n",
      "Connecting to tfhub.dev (tfhub.dev)|2607:f8b0:400a:800::200e|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://storage.googleapis.com/tfhub-modules/google/bert_uncased_L-12_H-768_A-12/1.tar.gz [following]\n",
      "--2020-12-29 16:32:29--  https://storage.googleapis.com/tfhub-modules/google/bert_uncased_L-12_H-768_A-12/1.tar.gz\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2607:f8b0:400e:c08::80, 2607:f8b0:400e:c04::80, 2607:f8b0:400e:c07::80, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2607:f8b0:400e:c08::80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 407795699 (389M) [application/x-tar]\n",
      "Saving to: ‘bert_uncased.tar.gz’\n",
      "\n",
      "bert_uncased.tar.gz 100%[===================>] 388.90M  4.39MB/s    in 86s     \n",
      "\n",
      "2020-12-29 16:33:55 (4.51 MB/s) - ‘bert_uncased.tar.gz’ saved [407795699/407795699]\n",
      "\n",
      "Bert Uncased downloaded and extracted into ./bert_uncased.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pickle as pck \n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "# download dataset\n",
    "!./get_aclImdb.sh"
   ]
  },
  {
   "source": [
    "# Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 256 # maximum number of tokens per review\n",
    "max_chars = 20 # maximum size of a token.\n",
    "n_samples = 300 # number of training instances"
   ]
  },
  {
   "source": [
    "Three helper methods to tokenize, remove stopwords, remove puntuation and convert to lowercase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_tfidf(path):\n",
    "\n",
    "    # Reviews will be loaded in sequence, i.e., first all negative s followed by all positives.\n",
    "    reviews, sentiments = [], []\n",
    "    for folder, sentiment in (('neg', 0), ('pos', 1)):\n",
    "        folder = os.path.join(path, folder)\n",
    "        for name in os.listdir(folder):\n",
    "            with open(os.path.join(folder, name), 'r') as reader:\n",
    "                text = reader.read()\n",
    "\n",
    "            reviews.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "\n",
    "    return reviews, sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training reviews: 25000 and labels: 25000\n",
      "Number of testing reviews: 25000 and labels: 25000\n"
     ]
    }
   ],
   "source": [
    "# Loaded dataset comes tokenized, lowercased and without both stopwords and punctuations\n",
    "\n",
    "train_path = os.path.join('data/aclImdb', 'train')\n",
    "reviews_train, sentiments_train = load_dataset_tfidf(train_path)\n",
    "print(f'Number of training reviews: {len(reviews_train)} and labels: {len(sentiments_train)}')\n",
    "\n",
    "test_path = os.path.join('data/aclImdb', 'test')\n",
    "reviews_test, sentiments_test = load_dataset_tfidf(test_path)\n",
    "print(f'Number of testing reviews: {len(reviews_test)} and labels: {len(sentiments_test)}')"
   ]
  },
  {
   "source": [
    "As seen previously this dataset may be huge to be digested in some environments. So let's limit the train dataset to the hyperparameter n_samples. The idea is to have a dataset with evenly distributed samples from negative and positive labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# BOW for Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_data(data, label):\n",
    "\n",
    "    print(len(label))\n",
    "    data_tmp = np.array(data)\n",
    "    label_tmp = np.asarray(label)\n",
    "    index_shuffle = np.random.permutation(data_tmp.shape[0])\n",
    "    data_tmp = data_tmp[index_shuffle]\n",
    "    label_tmp = label_tmp[index_shuffle]\n",
    "\n",
    "    return data_tmp.tolist(), label_tmp.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_sample_data(data, label, n_samples=1000):\n",
    "\n",
    "    data_tmp = np.array(data)\n",
    "    label_tmp = np.asarray(label)\n",
    "    index_shuffle = np.random.permutation(n_samples)\n",
    "    data_tmp = data_tmp[index_shuffle]\n",
    "    label_tmp = label_tmp[index_shuffle]\n",
    "\n",
    "    return data_tmp.tolist(), label_tmp.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25000\n25000\n"
     ]
    }
   ],
   "source": [
    "print(len(sentiments_train))\n",
    "reviews_train, sentiments_train = shuffle_and_sample_data(reviews_train, sentiments_train, n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = vectorizer.transform(reviews_train)\n",
    "train_y = np.array(sentiments_train)"
   ]
  },
  {
   "source": [
    "# Logistic Regression "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def fit(train_x, train_y):\n",
    "\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    try:\n",
    "        model.fit(train_x, train_y)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000, 74849) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "model = fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = vectorizer.transform(reviews_test)\n",
    "test_y = np.array(sentiments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression accucary is:  0.8832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score = accuracy_score(test_y, predictions)\n",
    "print(f'LogisticRegression accucary is: {accuracy_score: .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "\n",
    "# Making sure we have the correct tensorflow version\n",
    "assert tf.__version__ == \"1.15.2\", 'Correct tensorflow version is 1.15.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting TF sessions and setting it to keras backend\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "class BertEmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "        n_fine_tune_layer = 10,\n",
    "        pooling = 'mean',\n",
    "        bert_path = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1',\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        self.n_fine_tune_layer = n_fine_tune_layer\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "\n",
    "        super(BertEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_features):\n",
    "\n",
    "        self.bert_layer = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f'{self.name}_module')\n",
    "\n",
    "        print(type(self.bert_layer))\n",
    "\n",
    "        trainable_vars = self.bert_layer.variables\n",
    "        \n",
    "        if self.pooling == 'first':\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "        elif self.pooling == 'mean':\n",
    "            trainable_vars = [\n",
    "                var for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name \n",
    "                and not \"/pooler/\" in var.name]\n",
    "\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError('Undefined pooling type')\n",
    "\n",
    "        for i in range(self.n_fine_tune_layer):\n",
    "            trainable_layers.append(f'encoder/layer_{str(11 - i)}')\n",
    "\n",
    "        trainable_vars = [\n",
    "            var for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert_layer.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertEmbeddingLayer, self).build(input_features)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype='int32') for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
    "\n",
    "        if self.pooling == 'first':\n",
    "            pooled = self.bert_layer(\n",
    "                inputs=bert_inputs, signature='tokens', as_dict=True)['pooled_output']\n",
    "\n",
    "        elif self.pooling == 'mean':\n",
    "            result = self.bert_layer(\n",
    "                inputs=bert_inputs, signature='tokens', as_dict=True)['sequence_output']\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (tf.reduce_sum(m, axis=1, keep_dims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "\n",
    "        else:\n",
    "            raise NameError('Undefined pooling type')\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_fine_tune_layer': self.n_fine_tune_layer,\n",
    "            'pooling': self.pooling,\n",
    "            'bert_path': self.bert_path,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(300,)\n(300,)\n(300, 1)\n(300, 1)\n"
     ]
    }
   ],
   "source": [
    "def convert_data_to_bert_feature_format(x, y):\n",
    "\n",
    "    # Converted_data to np.array\n",
    "    converted_data = np.array(x, dtype=object)[:, np.newaxis]\n",
    "\n",
    "    return converted_data, np.array(y)\n",
    "\n",
    "reviews_train, sentiments_train = shuffle_and_sample_data(reviews_train, sentiments_train, n_samples)\n",
    "reviews_test, sentiments_test = shuffle_and_sample_data(reviews_test, sentiments_test, n_samples)\n",
    "\n",
    "data_train, labels_train = convert_data_to_bert_feature_format(reviews_train, sentiments_train)\n",
    "data_test, labels_test = convert_data_to_bert_feature_format(reviews_test, sentiments_test)\n",
    "\n",
    "print(labels_train.shape)\n",
    "print(labels_test.shape)\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_seq_length):\n",
    "\n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name='input_ids')\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name='input_masks')\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name='segment_ids')\n",
    "    bert_layer_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "    bert_output = BertEmbeddingLayer(n_fine_tune_layer=0)(bert_layer_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=bert_layer_inputs, outputs=output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "Converting examples to features: 100%|██████████| 300/300 [00:01<00:00, 256.03it/s]\n",
      "Converting examples to features: 100%|██████████| 300/300 [00:01<00:00, 261.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import kerasBert as kb\n",
    "import bert.tokenization as tk\n",
    "from tensorflow_hub import Module\n",
    "import pkg_resources as pkg\n",
    "import bert_utils as bu\n",
    "\n",
    "# Making sure we have install the correct libraries.\n",
    "assert pkg.get_distribution(\"bert-tensorflow\").version == '1.0.1', 'Change bert-tensorflow version to 1.0.1'\n",
    "\n",
    "vocab_file_path = '/home/baosiek/Development/deep_learning/sentiment_analysis/model/bert/assets/vocab.txt'\n",
    "tokenizer = bu.create_tokenizer(vocab_file_path)\n",
    "\n",
    "train_examples = kb.convert_text_to_examples(data_train, labels_train)\n",
    "test_examples = kb.convert_text_to_examples(data_test, labels_test)\n",
    "\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels) = kb.convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_tokens)\n",
    "\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels) = kb.convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('model/bert/bert_model.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Build and training model!\n",
      "<class 'tensorflow_hub.module.Module'>\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:tensorflow:From /tmp/tmp5rbodbsj.py:38: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /tmp/tmp5rbodbsj.py:38: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/baosiek/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_embedding_layer (BertEmbed (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_embedding_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 197,121\n",
      "Non-trainable params: 110,104,890\n",
      "__________________________________________________________________________________________________\n",
      "Train on 300 samples, validate on 300 samples\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 443s 1s/sample - loss: 0.6827 - acc: 0.5633 - val_loss: 0.4159 - val_acc: 0.9733\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 442s 1s/sample - loss: 0.5775 - acc: 0.6733 - val_loss: 0.4351 - val_acc: 0.8900\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 445s 1s/sample - loss: 0.5433 - acc: 0.7367 - val_loss: 0.7216 - val_acc: 0.5467\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 439s 1s/sample - loss: 0.4671 - acc: 0.7900 - val_loss: 0.5514 - val_acc: 0.7267\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 425s 1s/sample - loss: 0.4392 - acc: 0.8100 - val_loss: 0.5224 - val_acc: 0.7567\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9816526d626d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model/bert/bert_model.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/bert/train_history_dict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading trained model!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Will train only if bert_model.h5 does not exi\n",
    "if not os.path.exists('model/bert/bert_model.hdf5'):\n",
    "    print('Build and training model!')\n",
    "    model = build_model(max_tokens)\n",
    "\n",
    "    # Initialize keras.backend==tensorflow session\n",
    "    initialize_vars(sess)\n",
    "\n",
    "    # Fitting the model\n",
    "    history = model.fit([train_input_ids, train_input_masks, train_segment_ids], train_labels, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels), epochs=5, batch_size=32)\n",
    "\n",
    "    # Saving history dictionary\n",
    "    history_dict = history.history\n",
    "    tf.keras.models.save_model(model, 'model/bert/bert_model.hdf5')\n",
    "    with open('model/bert/train_history_dict', 'wb') as file_pi:\n",
    "        pck.dump(history_dict, file_pi)\n",
    "else:\n",
    "    print('Loading trained model!')\n",
    "    model = tf.keras.models.load_model('model/bert/bert_model.hdf5', {'BertEmbeddingLayer': BertEmbeddingLayer})\n",
    "    with open(\"model/bert/train_history_dict\", \"rb\") as fp:\n",
    "        history_dict = pck.load(fp)\n",
    "\n",
    "# ploting history for accuracy and loss\n",
    "plt.plot(history_dict['acc'])\n",
    "plt.plot(history_dict['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# ploting history for loss\n",
    "plt.plot(history_dict['loss'])\n",
    "plt.plot(history_dict['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Now lets see examples working\n",
    "print('--------------------------------')\n",
    "print('Applying classifier on examples:')\n",
    "print('--------------------------------')\n",
    "\n",
    "text_examples = ['The movie was excellent', 'The movie was horrible', 'Oh! What a magnificent movie.',\n",
    "'Poor casting. Excellent acting. Excelent soudtrack. Regular movie.']\n",
    "\n",
    "# Here I am classifying messages one at a time just to work with different helper functions.\n",
    "# This could be dealt as we did with training however,i.e., using kb.convert_text_to_examples\n",
    "# and then kb.convert_examples_to_features.\n",
    "\n",
    "for text_example in text_examples:\n",
    "    # example should be an instance of class InputExample in keras-bert.py\n",
    "    example = kb.InputExample(None, text_example)\n",
    "\n",
    "    # helpser methos from keras-bert.py\n",
    "    (example_input_id, example_input_mask, example_segment_id, example_label) = kb.convert_single_example(tokenizer, example, max_seq_length=max_tokens)\n",
    "\n",
    "    # predicting\n",
    "    predict = model.predict([np.reshape(example_input_id, (1, len(example_input_id))), np.reshape(example_input_mask, (1, len(example_input_mask))), np.reshape(example_segment_id, (1, len(example_segment_id)))])\n",
    "    if predict[0] < 0.5:\n",
    "        print(f'{example.text_a}: is NEGATIVE!')\n",
    "    else:\n",
    "        print(f'{example.text_a}: is POSITIVE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}