{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bittlcondac3b95453ae5d46cea17442527270ea62",
   "display_name": "Python 3.7.9 64-bit ('tl': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h1><center>Sentiment Analysis Classifier with BERT</center></h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This project is based on Manning's book \"Transfer Learning for NLP\" (chapter 2).\n",
    "The goal here is:\n",
    "\n",
    "1. Curate a dataset with reviews from imdb classic dataset\n",
    "2. Create a Pandas dataframe from iy\n",
    "3. Create a simple bag-of-words model from the above content. Simple because it is based on term frequency (tf) only.\n",
    "4. Choose one baseline classifier from Logistic Regression and Gradient Boosting Machine\n",
    "5. Accuracy is the metric of choice as the dataset is balanced and consists of two classes\n",
    "6. Train a SentimentAnalysis classifier based on BERT embeddings\n",
    "\n",
    "But before starting let's make sure we have the correct libraries versions installed, namelly tensorflow and bert-tensorflow."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tensorflow==1.15.2 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.15.2)\n",
      "Requirement already satisfied: bert-tensorflow==1.0.1 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.19.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (3.13.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.11.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.15.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.31.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.35.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.15.2->-r requirements.txt (line 1)) (50.3.1.post20201107)\n",
      "Requirement already satisfied: h5py in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.2->-r requirements.txt (line 1)) (2.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (0.16.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/baosiek/anaconda3/envs/tl/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 1)) (3.4.0)\n"
     ]
    }
   ]
  },
  {
   "source": [
    "Now we will import required Python libraries and the dataset. To download the dataset I will use the bash script get_aclImdb.sh. It downloads and extracts the compressed archive into ./data/aclImdb. It requires execution privilege (sudo chmod +x get_aclImdb.sh)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "aclImdb already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pickle as pck \n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "# download dataset\n",
    "!./get_aclImdb.sh"
   ]
  },
  {
   "source": [
    "# Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 256 # maximum number of tokens per review\n",
    "max_chars = 20 # maximum size of a token.\n",
    "n_samples = 1000 # number of training instances"
   ]
  },
  {
   "source": [
    "Three helper methods to tokenize, remoce stopwords, remove puntuation and convert to lowercase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_tfidf(path):\n",
    "\n",
    "    # Reviews will be loaded in sequence, i.e., first all negative s followed by all positives.\n",
    "    reviews, sentiments = [], []\n",
    "    for folder, sentiment in (('neg', 0), ('pos', 1)):\n",
    "        folder = os.path.join(path, folder)\n",
    "        for name in os.listdir(folder):\n",
    "            with open(os.path.join(folder, name), 'r') as reader:\n",
    "                text = reader.read()\n",
    "\n",
    "            reviews.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "\n",
    "    return reviews, sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training reviews: 25000 and labels: 25000\n",
      "Number of testing reviews: 25000 and labels: 25000\n"
     ]
    }
   ],
   "source": [
    "# Loaded dataset comes tokenized, lowercased and without both stopwords and punctuations\n",
    "\n",
    "train_path = os.path.join('data/aclImdb', 'train')\n",
    "reviews_train, sentiments_train = load_dataset_tfidf(train_path)\n",
    "print(f'Number of training reviews: {len(reviews_train)} and labels: {len(sentiments_train)}')\n",
    "\n",
    "test_path = os.path.join('data/aclImdb', 'test')\n",
    "reviews_test, sentiments_test = load_dataset_tfidf(test_path)\n",
    "print(f'Number of testing reviews: {len(reviews_test)} and labels: {len(sentiments_test)}')"
   ]
  },
  {
   "source": [
    "As seen previously this dataset may be huge to be digested in some environments. So let's limit the train dataset to the hyperparameter n_samples. The idea is to have a dataset with evenly distributed samples from negative and positive labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# BOW for Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_data(data, label):\n",
    "\n",
    "    data_tmp = np.array(data)\n",
    "    label_tmp = np.array(label)\n",
    "    index_shuffle = np.random.permutation(data_tmp.shape[0])\n",
    "    data_tmp = data_tmp[index_shuffle]\n",
    "    label_tmp = label_tmp[index_shuffle]\n",
    "\n",
    "    return data_tmp.tolist(), label_tmp.tolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train, sentiments_train = shuffle_data(reviews_train, sentiments_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = vectorizer.transform(reviews_train)\n",
    "train_y = np.array(sentiments_train)"
   ]
  },
  {
   "source": [
    "# Logistic Regression "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def fit(train_x, train_y):\n",
    "\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    try:\n",
    "        model.fit(train_x, train_y)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = vectorizer.transform(reviews_test)\n",
    "test_y = np.array(sentiments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression accucary is:  0.8832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score = accuracy_score(test_y, predictions)\n",
    "print(f'LogisticRegression accucary is: {accuracy_score: .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}